{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T14:24:27.139500Z",
     "start_time": "2021-08-03T14:24:24.096163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cbobed\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Author: Carlos Bobed\n",
    "# Date: Nov 2020\n",
    "# Comments: Code to partition a transaction database according to a \n",
    "# clustering using its embedding representations\n",
    "# Modifications:\n",
    "###############################################################################\n",
    "\n",
    "import gensim, logging, os, sys, gzip\n",
    "import time\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',filename='word2vec.out', level=logging.INFO)\n",
    "                        \n",
    "## method to read for the Vreeken's codetable format \n",
    "## we don't need it to be a generator \n",
    "## we do label each code and honor the order in the code table (length, support, lexicographical)\n",
    "## following Pierre's suggestion, we keep track of the codes and the transaction IDs \n",
    "def read_codetable(filename, load_all): \n",
    "    codes = {}\n",
    "    label = 0 \n",
    "    with open(filename, mode='rt', encoding='UTF-8') as file: \n",
    "        for line in file: \n",
    "            item_line = list(filter(None, line.rstrip('\\n').split(' ')))\n",
    "            ## only_used => those codes whose usage is > 0\n",
    "            ## we get the last token, check whether it ends with )\n",
    "            ## then, we get exactly the contents and check whether the first \n",
    "            ## component is different from 0\n",
    "            if (item_line[-1].endswith(')')): \n",
    "                usage,support = item_line[-1][1:-1].split(',')\n",
    "                if (load_all or int(usage) != 0):\n",
    "                    codes[label]={'code': item_line[:-1], 'usage':int(usage), 'support':int(support)}\n",
    "                    label+=1\n",
    "    return codes        \n",
    "\n",
    "## to keep track of the transactions id in the database, we read them in a different method\n",
    "def read_database_db (filename): \n",
    "    transactions = {}\n",
    "    label = 0\n",
    "    with open(filename, mode='rt', encoding='UTF-8') as file: \n",
    "        for line in open(filename, mode='rt', encoding='UTF-8'): \n",
    "            if (line.split(':')[0].isnumeric()): \n",
    "                aux = line.split(':')[1].rstrip('\\n')\n",
    "                words = filter(None,aux.split(' '))\n",
    "                transactions[label] = list(words)\n",
    "                label+=1\n",
    "    return transactions\n",
    "\n",
    "def read_database_dat(filename): \n",
    "    transactions = {}\n",
    "    label = 0\n",
    "    with open(filename, mode='rt', encoding='UTF-8') as file: \n",
    "        for line in open(filename, mode='rt', encoding='UTF-8'): \n",
    "            aux = line.rstrip('\\n')\n",
    "            words = filter(None,aux.split(' '))\n",
    "            transactions[label] = list(words)\n",
    "            label+=1\n",
    "    return transactions\n",
    "\n",
    "## read information from the analysis to get back to the .dat file and select them\n",
    "## we can cluster them according just to the items, or to the transactions themselves\n",
    "\n",
    "def read_analysis_table (filename): \n",
    "    table = {}\n",
    "    with open(filename, mode='rt', encoding='UTF-8') as file: \n",
    "        # skip the first  lines\n",
    "        for i in range(15): \n",
    "            file.readline()\n",
    "        current_line = file.readline()\n",
    "        while (current_line != \"\\n\"): \n",
    "            aux = current_line.split()[0].split('=>')\n",
    "            table[int(aux[0])] = int(aux[1])\n",
    "            current_line = file.readline()\n",
    "    return table\n",
    "\n",
    "## convert a transaction in Vreken to the original item name\n",
    "def convert_transaction_db_to_dat (transaction, table): \n",
    "    return sorted([table[int(item)] for item in transaction])\n",
    "\n",
    "def convert_database_db_to_dat(database, table, output_filename): \n",
    "    with open(output_filename, mode='wt', encoding='UTF-8') as file: \n",
    "        for i in database: \n",
    "            for item in convert_transaction_db_to_dat(database[i], table): \n",
    "                file.write(f'{item} ')\n",
    "            file.write('\\n')\n",
    "\n",
    "## split a database regarding a cluster of items that has been calculated \n",
    "## either at item embedding or at transaction embedding level\n",
    "## for the time being everything is done in memory \n",
    "## TODO: process each line at a time\n",
    "def split_database_items(database, database_name, clusters): \n",
    "    ## we create an in-memory database for each cluster\n",
    "    ## the items currently are stored in the DB as strings as they are tokens for the \n",
    "    ## word embedding\n",
    "    ## the clusters of items can be just sets of integers for speed up reasons\n",
    "    k=len(clusters)\n",
    "    in_mem_splitting = {label:[] for label in clusters}\n",
    "    for i in database: \n",
    "        aux_set = set()\n",
    "        [aux_set.add(int(item)) for item in database[i]]\n",
    "        ## NOT THE MOST EFFICIENT WAY TO DO IT!!!! \n",
    "        ## to speed up this: we should create an inverted index\n",
    "        [in_mem_splitting[label].append(database[i]) for label in clusters if len(aux_set.intersection(clusters[label])) != 0]\n",
    "    \n",
    "    for label in clusters: \n",
    "        with open(os.path.join('databases', database_name[:-4]+'_'+label+'_k'+str(k)+'.dat'), mode='wt', encoding='UTF-8') as file: \n",
    "            for trans in in_mem_splitting[label]: \n",
    "                [file.write(f'{item} ') for item in trans]\n",
    "                file.write('\\n')\n",
    "                \n",
    "def split_database_transactions (database_name, clusters): \n",
    "    ## the splitting has been already done, and they just give clusters of transactions\n",
    "    k=len(clusters)\n",
    "    for label in clusters: \n",
    "        with open(os.path.join('databases', database_name[:-4]+'_'+str(label)+'_k'+str(k)+'.dat'), mode='wt', encoding='UTF-8') as file: \n",
    "            for trans in clusters[label]: \n",
    "                [file.write(f'{item} ') for item in trans]\n",
    "                file.write('\\n')\n",
    "                \n",
    "def split_database_transactions_translating (database_name, clusters, table): \n",
    "    ## the splitting has been already done, and they just give clusters of transactions\n",
    "    k=len(clusters)\n",
    "    for label in clusters: \n",
    "        with open(os.path.join('databases', database_name[:-4]+'_'+str(label)+'_k'+str(k)+'.dat'), mode='wt', encoding='UTF-8') as file: \n",
    "            for trans in clusters[label]: \n",
    "                [file.write(f'{table[int(item)]} ') for item in trans]\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T14:24:33.271021Z",
     "start_time": "2021-08-03T14:24:33.261950Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_name='databases'\n",
    "database_name = 'adult'\n",
    "database_filename = database_name + '.db'\n",
    "database_analysis_filename = database_filename + '.analysis.txt'\n",
    "database_model_filename = database_filename + '.vect'\n",
    "codetable_filename = database_name+'-latest-SLIM.ct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T14:24:36.764378Z",
     "start_time": "2021-08-03T14:24:36.748379Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## convert all the .db files into .dat \n",
    "# for db in [filename for filename in os.listdir('databases') if filename.endswith('.db')]:\n",
    "#     trans_table = read_analysis_table(os.path.join('databases', db + '.analysis.txt'))\n",
    "#     db_database = read_database_db(os.path.join('databases', db))\n",
    "#     convert_database_db_to_dat(db_database, trans_table, os.path.join('databases', db[:-3]+'.dat')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T14:24:38.844813Z",
     "start_time": "2021-08-03T14:24:37.981670Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'databases\\\\adult.db.vect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-769a5e8f0721>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatabase_model_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mlabelled_vects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \"\"\"\n\u001b[0;32m   1921\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1922\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1923\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1924\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \"\"\"\n\u001b[1;32m-> 1457\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1458\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'databases\\\\adult.db.vect'"
     ]
    }
   ],
   "source": [
    "### Clustering functions and options \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import math \n",
    "import os\n",
    "\n",
    "model = Word2Vec.load(os.path.join(dir_name, database_model_filename))\n",
    "labelled_vects = {int(x): model.wv[x] for x in model.wv.vocab}\n",
    "\n",
    "def calculate_centroids (model, labelled_transactions): \n",
    "\n",
    "#     initial code\n",
    "#     centroids = []\n",
    "#     for transaction in transaction_list: \n",
    "#         words = [model.wv[it] for it in transaction]\n",
    "#         centroids.append(np.mean(words, axis=0))\n",
    "#     return centroids\n",
    "\n",
    "    # more pythonic way\n",
    "    return { label: np.mean([model.wv[it] for it in  labelled_transactions[label]], axis=0) for label in labelled_transactions}\n",
    "\n",
    "\n",
    "def calculate_normalized_centroids(model, labelled_transactions): \n",
    "    dim = model.wv[labelled_transactions[0][0]].shape[0]\n",
    "    return { label: preprocessing.normalize(np.mean([model.wv[it] for it in labelled_transactions[label]], axis=0).reshape(1,dim), norm='l2').reshape(dim,) \n",
    "                                            for label in labelled_transactions}\n",
    "\n",
    "### NOTES_ \n",
    "### using the overall probability of each item as weight to calculate the centroid \n",
    "### lead to decisions which partition the codes further (in the histograms I've observed that \n",
    "### the freq of lengths of the codes gets higher in the shorter regions)\n",
    "### going for a soft clustering technique (considering fuzzy k-means, or hdbscan)\n",
    "\n",
    "def calculate_weighted_centroids(model, labelled_transactions): \n",
    "    # model.wv.vocab['1'].count\n",
    "    # I was going to get the softmax derived weights but it would add some dependency that we don't want \n",
    "    # to explore right now\n",
    "    # for the time being, let's focus on the global probability of each item being part of a transaction\n",
    "    \n",
    "#     total_sum = sum([model.wv.vocab[it].count for it in model.wv.vocab])\n",
    "#     e_values = {it: math.pow(math.e, model.wv.vocab[it].count / total_sum) for it in model.wv.vocab}\n",
    "#     total_e_values = sum([evalues[it] for it in e_values])\n",
    "#     weights = {it:e_values[it] / total_e_values for it in e_values}\n",
    "    \n",
    "    result = {}\n",
    "    weights = {it: model.wv.vocab[it].count/len(labelled_transactions) for it in model.wv.vocab}\n",
    "    for label in labelled_transactions: \n",
    "        item_vect = []\n",
    "        weight_vect = []\n",
    "        for it in labelled_transactions[label]: \n",
    "            item_vect.append(model.wv[it])\n",
    "            weight_vect.append(weights[it])\n",
    "        result[label] = np.average(item_vect, weights=np.array(weight_vect, dtype='float32'), axis=0)\n",
    "    return result\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "if (database_filename.endswith('.db')): \n",
    "    database_transactions = read_database_db(os.path.join('.', dir_name, database_filename))\n",
    "elif (database_filename.endswith('.dat')): \n",
    "    database_transactions = read_database_dat(os.path.join('.', dir_name, database_filename))\n",
    "centroids = calculate_centroids(model, database_transactions)\n",
    "weighted_centroids = calculate_weighted_centroids(model, database_transactions)\n",
    "normalized_centroids = calculate_normalized_centroids(model, database_transactions)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(centroids[0].shape)\n",
    "print(weighted_centroids[0].shape)\n",
    "print(normalized_centroids[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.820908546447754\n"
     ]
    }
   ],
   "source": [
    "## I was going to use scikit directly, but I saw this post\n",
    "## kudos for him https://towardsdatascience.com/k-means-8x-faster-27x-lower-error-than-scikit-learns-in-25-lines-eaedc7a3a0c8\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "## we compute the kmeans of the transactions\n",
    "database_as_nparray = np.array([centroids[c] for c in centroids])\n",
    "trans_kmeans = faiss.Kmeans(d=database_as_nparray.shape[1], k=8, niter = 2000, nredo=10)\n",
    "trans_kmeans.train(database_as_nparray)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5e3ec0fb8d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdatabase_as_nparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalized_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalized_centroids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrans_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase_as_nparray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnredo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrans_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_as_nparray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdf2vecEnv/lib/python3.7/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mngpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_cpu_to_all_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mclus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_float_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdf2vecEnv/lib/python3.7/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36mreplacement_train\u001b[0;34m(self, x, index, weights)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplacement_train_encoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdf2vecEnv/lib/python3.7/site-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n, x, index, x_weights)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClustering_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_encoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## I was going to use scikit directly, but I saw this post\n",
    "## kudos for him https://towardsdatascience.com/k-means-8x-faster-27x-lower-error-than-scikit-learns-in-25-lines-eaedc7a3a0c8\n",
    "\n",
    "#normalizing the vectors, we will have the spherical k-means\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "start = time.time()\n",
    "## we compute the kmeans of the transactions\n",
    "database_as_nparray = np.array([normalized_centroids[c] for c in normalized_centroids])\n",
    "trans_kmeans = faiss.Kmeans(d=database_as_nparray.shape[1], k=8, niter = 2000, nredo=10)\n",
    "trans_kmeans.train(database_as_nparray)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Might be too far expensive for our purposes\n",
    "\n",
    "import hdbscan\n",
    "import time \n",
    "start = time.time() \n",
    "\n",
    "trans_hdbscan = hdbscan.HDBSCAN(min_cluster_size=1000, prediction_data=True).fit(database_as_nparray)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T14:27:06.735724Z",
     "start_time": "2021-08-03T14:27:06.486890Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.datasets.samples_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-01dc2779dc07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_extensions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfuzzy_kmeans\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mske\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\sklearn_extensions\\fuzzy_kmeans\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_extensions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuzzy_kmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMedians\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFuzzyKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\rdf2vecEnv\\lib\\site-packages\\sklearn_extensions\\fuzzy_kmeans\\kmeans.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanhattan_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.datasets.samples_generator'"
     ]
    }
   ],
   "source": [
    "from sklearn_extensions import fuzzy_kmeans as ske\n",
    "import time \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "trans_fuzzy_k_means = ske.FuzzyKMeans(k=8, max_iter=300)\n",
    "trans_fuzzy_k_means.fit(database_as_nparray)\n",
    "trans_fuzzy_k_means.predict(database_as_nparray)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered = np.transpose([centroids[c] for c in sorted(centroids)])\n",
    "np.array_equal(ordered[:,1500], centroids[1500])\n",
    "ordered[:,1500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfuzzy as fuzz\n",
    "\n",
    "start = time.time() \n",
    "f\n",
    "# as we don't have the predict feature, we have to keep track of the order \n",
    "database_as_np_array_fuzzy = np.transpose(np.array([centroids[c] for c in sorted(centroids)]))\n",
    "\n",
    "cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans( database_as_np_array_fuzzy, 8, 1.5, error=0.005, maxiter=1000, init=None)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 67557)\n",
      "20288\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "idxs = np.argsort(u,axis=0)\n",
    "print(idxs.shape)\n",
    "max_values = [ u[idxs[:,idx][-1], idx] for idx in range(idxs.shape[1])  ]\n",
    "second_max_values = [ u[idxs[:,idx][-2], idx] for idx in range(idxs.shape[1])  ]\n",
    "\n",
    "count=0\n",
    "for i in max_values: \n",
    "    if i>0.5: \n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "count=0\n",
    "for i in second_max_values: \n",
    "    if i>0.25: \n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we add the fuzzy k-means tagging \n",
    "## beware we have made sure of the ordering and it comes transposed [:,i] is the array of k grades of the i-th transaction\n",
    "\n",
    "def fuzzy_clustering_top_k(database, trans_vects, top_k, fuzzy_grades):\n",
    "    fuzzy_clusters = {}\n",
    "    for i in range(fuzzy_grades.shape[0]): \n",
    "        fuzzy_clusters[i] = []\n",
    "\n",
    "    for c in sorted(trans_vects): \n",
    "        idx = np.argsort(fuzzy_grades[:,c], axis=0)\n",
    "        for i in range(-1, -1 -top_k, -1): \n",
    "            fuzzy_clusters[idx[i]].append(database[c])\n",
    "    return fuzzy_clusters\n",
    "\n",
    "def fuzzy_clustering_threshold(database, trans_vects, threshold, fuzzy_grades):\n",
    "    fuzzy_clusters = {}\n",
    "    for i in range(fuzzy_grades.shape[0]): \n",
    "        fuzzy_clusters[i] = []\n",
    "\n",
    "    for c in sorted(trans_vects): \n",
    "        idx = np.argsort(fuzzy_grades[:,c], axis=0)\n",
    "        fuzzy_clusters[idx[-1]].append(database[c])\n",
    "        if fuzzy_grades[:,c][idx[-2]] >= threshold: \n",
    "            fuzzy_clusters[idx[-2]].append(database[c])\n",
    "    return fuzzy_clusters\n",
    "\n",
    "f1 = fuzzy_clustering_top_k(database_transactions, centroids, 1, u)\n",
    "f2 = fuzzy_clustering_top_k(database_transactions, centroids, 2, u)\n",
    "f4 = fuzzy_clustering_top_k(database_transactions, centroids, 4, u)\n",
    "\n",
    "f020 = fuzzy_clustering_threshold(database_transactions, centroids, 0.20, u)\n",
    "f025 = fuzzy_clustering_threshold(database_transactions, centroids, 0.25, u)\n",
    "f030 = fuzzy_clustering_threshold(database_transactions, centroids, 0.30, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10743 -- 11505 -- 11537 || 10743 -- 10756 -- 10847\n",
      "108 -- 11490 -- 63253 || 108 -- 108 -- 4239\n",
      "151 -- 12520 -- 64495 || 151 -- 151 -- 3186\n",
      "10557 -- 14117 -- 14177 || 10562 -- 10659 -- 10928\n",
      "11188 -- 12144 -- 12168 || 11188 -- 11205 -- 11304\n",
      "12103 -- 33294 -- 48997 || 12103 -- 12103 -- 12115\n",
      "11122 -- 26664 -- 42119 || 11122 -- 11122 -- 11146\n",
      "11585 -- 13380 -- 13482 || 11592 -- 11629 -- 11795\n"
     ]
    }
   ],
   "source": [
    "for i in f1: \n",
    "    print(f'{len(f1[i])} -- {len(f2[i])} -- {len(f4[i])} || {len(f030[i])} -- {len(f025[i])} -- {len(f020[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12075465, 0.50925817, 0.06249488, 0.02634021, 0.09281569,\n",
       "       0.05153289, 0.06259125, 0.07421227])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777.3660278320312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_as_nparray = np.array([labelled_vects[label] for label in labelled_vects])\n",
    "\n",
    "items_kmeans = faiss.Kmeans(d=items_as_nparray.shape[1], k=8, niter = 300, nredo=10)\n",
    "items_kmeans.train(items_as_nparray)\n",
    "\n",
    "# ## we compute the clustering of the database according to the kmeans of the transaction\n",
    "# items_D, items_I = trans_kmeans.index.search(items_as_nparray, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(centroids[0], centroids[0].reshape(1,200)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can have then 3 different partitions of the database: \n",
    "## clustered according to the items k-means\n",
    "## clustered according to the items obtained from the \"flattened\" transaction k-means\n",
    "## clustered directly from the transaction k-means\n",
    "\n",
    "## we build the clusters accordingly from the calculated k-means of their vectors \n",
    "## done in this way for readability purposes, we can speed up this process by \n",
    "## sharing orders \n",
    "def item_labelling_from_item_vects (item_vects, items_kmeans): \n",
    "    #labelled_vects = {int(x): model.wv[x] for x in model.wv.vocab}\n",
    "    cluster = {}\n",
    "    base_label_item = 'item_clust_'\n",
    "    for i in item_vects: \n",
    "        item_D, item_I = items_kmeans.index.search(item_vects[i].reshape(1,200), 1)\n",
    "        current_label = base_label_item+str(item_I[0,0])\n",
    "        if current_label not in cluster: \n",
    "            cluster[current_label] = set()\n",
    "        cluster[current_label].add(i)\n",
    "    return cluster\n",
    "\n",
    "## we build the clusters accordingly from the items in the transactions belonging to a \n",
    "## cluster\n",
    "def item_labelling_from_trans_vects (database, trans_vects, trans_kmeans): \n",
    "# from read_database_*\n",
    "#     transactions[label] = list(words)\n",
    "#     label+=1\n",
    "    \n",
    "    cluster = {}\n",
    "    base_label_trans_item = 'trans_item_clust_'\n",
    "    for t in trans_vects: \n",
    "        trans_D, trans_I = trans_kmeans.index.search(trans_vects[t].reshape(1,200), 1)\n",
    "        current_label = base_label_trans_item+str(trans_I[0,0])\n",
    "        if current_label not in cluster: \n",
    "            cluster[current_label] = set()\n",
    "        ## centroids/trans_vects share the same ids as database \n",
    "        for item in database[t]: \n",
    "            cluster[current_label].add(int(item))\n",
    "    return cluster\n",
    "\n",
    "## finally, we build the clusters accordingly from the transaction vectors \n",
    "def trans_labelling_from_trans_vects (database, trans_vects, trans_kmeans): \n",
    "    cluster = {}\n",
    "    base_label_trans_item = 'trans_clust_'\n",
    "    for t in trans_vects: \n",
    "        trans_D, trans_I = trans_kmeans.index.search(trans_vects[t].reshape(1,200), 1)\n",
    "        current_label = base_label_trans_item+str(trans_I[0,0])\n",
    "        if current_label not in cluster: \n",
    "            cluster[current_label] = []\n",
    "        ## centroids/trans_vects share the same ids as database \n",
    "        cluster[current_label].append(database[t])\n",
    "    return cluster\n",
    "\n",
    "def trans_labelling_from_trans_vects_normalizing (database, trans_vects, trans_kmeans): \n",
    "    cluster = {}\n",
    "    base_label_trans_item = 'trans_clust_'\n",
    "    for t in trans_vects: \n",
    "        trans_D, trans_I = trans_kmeans.index.search(preprocessing.normalize(trans_vects[t].reshape(1,200)), 1)\n",
    "        current_label = base_label_trans_item+str(trans_I[0,0])\n",
    "        if current_label not in cluster: \n",
    "            cluster[current_label] = []\n",
    "        ## centroids/trans_vects share the same ids as database \n",
    "        cluster[current_label].append(database[t])\n",
    "    return cluster\n",
    "\n",
    "import random\n",
    "## We randomly split transaction database \n",
    "def trans_labelling_random(database, k): \n",
    "    cluster={}\n",
    "    ## beware: database is a dict, not a list \n",
    "    idx_list = [i for i in range (len(database))]\n",
    "    size = int(math.ceil(len(idx_list)/k))\n",
    "    print(f'size of shuffle: {len(idx_list)}')\n",
    "    print(f'Partitioning DB of {len(idx_list)} in {k} sets of {size} elements')\n",
    "    base_label='rand_clust_'\n",
    "    random.shuffle(idx_list)\n",
    "    shuffled = [idx_list[i::k] for i in range(k)]\n",
    "    print(f'size of shuffle: {len(shuffled)}')\n",
    "    for (i,vect) in enumerate([ idx_list[i::k] for i in range(k)]):\n",
    "        current_label = base_label+str(i)\n",
    "        if current_label not in cluster: \n",
    "            cluster[current_label] = []\n",
    "        for j in vect: \n",
    "            cluster[current_label].append(database[j])\n",
    "    for cl in enumerate(cluster): \n",
    "        print(f'{cl[0]} - {cl[1]} -- len {len(cluster[cl[1]])}')\n",
    "    return cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random \n",
    "\n",
    "# l = [i for i in range(100)]\n",
    "\n",
    "# def partition (list_in, n):\n",
    "#     random.shuffle(list_in)\n",
    "#     return [list_in[i::n] for i in range(n)]\n",
    "\n",
    "# partition(l,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_cluster = item_labelling_from_item_vects(labelled_vects, items_kmeans)\n",
    "# trans_item_cluster = item_labelling_from_trans_vects(database_transactions, centroids, trans_kmeans)\n",
    "trans_cluster = trans_labelling_from_trans_vects(database_transactions, centroids, trans_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chessBig.db\n",
      "chessBig_ord_200d_k8.dat\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7cf2fd7109f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# split_database_items(database_transactions, database_filename, trans_item_cluster)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msplit_database_transactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlocal_database_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msplit_database_transactions_translating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_database_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation_table_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ed5b7a63545a>\u001b[0m in \u001b[0;36msplit_database_transactions_translating\u001b[0;34m(database_name, clusters, table)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_k'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{table[int(item)]} '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ed5b7a63545a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_k'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{table[int(item)]} '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 64"
     ]
    }
   ],
   "source": [
    "local_database_filename=database_filename[:-3]+'_ord_200d_k8.dat'\n",
    "print(database_filename)\n",
    "print(local_database_filename)\n",
    "translation_table_test = read_analysis_table(os.path.join('databases',  database_analysis_filename))\n",
    "\n",
    "# split_database_items(database_transactions, database_filename, item_cluster)\n",
    "# split_database_items(database_transactions, database_filename, trans_item_cluster)\n",
    "split_database_transactions(\"test_\"+local_database_filename, trans_cluster)\n",
    "split_database_transactions_translating(local_database_filename, trans_cluster, translation_table_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult.db\n",
      "adult_rand_k8.dat\n",
      "size of shuffle: 48842\n",
      "Partitioning DB of 48842 in 8 sets of 6106 elements\n",
      "size of shuffle: 8\n",
      "0 - rand_clust_0 -- len 6106\n",
      "1 - rand_clust_1 -- len 6106\n",
      "2 - rand_clust_2 -- len 6105\n",
      "3 - rand_clust_3 -- len 6105\n",
      "4 - rand_clust_4 -- len 6105\n",
      "5 - rand_clust_5 -- len 6105\n",
      "6 - rand_clust_6 -- len 6105\n",
      "7 - rand_clust_7 -- len 6105\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "108",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-39eac79a3f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtranslation_table_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_analysis_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdatabase_analysis_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrand_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_labelling_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_transactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msplit_database_transactions_translating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_database_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation_table_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ed5b7a63545a>\u001b[0m in \u001b[0;36msplit_database_transactions_translating\u001b[0;34m(database_name, clusters, table)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_k'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{table[int(item)]} '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ed5b7a63545a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_k'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{table[int(item)]} '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 108"
     ]
    }
   ],
   "source": [
    "## Random partitioning\n",
    "local_database_filename=database_filename[:-3]+'_rand_k8.dat'\n",
    "print(database_filename)\n",
    "print(local_database_filename)\n",
    "translation_table_test = read_analysis_table(os.path.join('databases',  database_analysis_filename))\n",
    "rand_cluster = trans_labelling_random(database_transactions, 8)\n",
    "split_database_transactions_translating(local_database_filename, trans_cluster, translation_table_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, if the items are not sparsely distributed the item-base clustering seems easily go to complete databases (if you have an item that's very frequent it's going to \"infect\" all of the transactions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is just testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 10, 1: 13, 2: 15, 3: 1, 4: 19, 5: 18, 6: 17, 7: 14, 8: 11, 9: 16, 10: 12, 11: 2, 12: 6, 13: 5, 14: 9, 15: 3, 16: 7, 17: 8, 18: 4}\n"
     ]
    }
   ],
   "source": [
    "translation_table_test = read_analysis_table(os.path.join('databases',  database_analysis_filename))\n",
    "print(translation_table_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'databases/iris.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2ca1ae8709a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat_database_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_database_dat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_filename_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-9fe16517e92b>\u001b[0m in \u001b[0;36mread_database_dat\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtransactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'databases/iris.dat'"
     ]
    }
   ],
   "source": [
    "dat_database_test = read_database_dat(os.path.join('databases', database_filename_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_database_test = read_database(os.path.join('databases', database_filename))\n",
    "convert_database_db_to_dat(db_database_test, translation_table_test , os.path.join('databases', 'testing-test.dat') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster={'primer':set(range(18)), 'second':set(range(18,19))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primer': {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17},\n",
       " 'second': {18}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1]\n",
      "[150, 2]\n"
     ]
    }
   ],
   "source": [
    "split_database_items(db_database_test, 'test-test', cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster={'primer':[db_database_test[i] for i in range(80)], 'second':[db_database_test[i] for i in range(80, len(db_database_test))]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_database_transactions('test-test', cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db_database_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
